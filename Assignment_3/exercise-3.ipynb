{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MoidHuda/HLCV_53"
      ],
      "metadata": {
        "id": "e0Phz4726v0S",
        "outputId": "1ed20d16-6723-4d42-f0bb-e642de16371c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HLCV_53'...\n",
            "remote: Enumerating objects: 311, done.\u001b[K\n",
            "remote: Counting objects: 100% (311/311), done.\u001b[K\n",
            "remote: Compressing objects: 100% (275/275), done.\u001b[K\n",
            "remote: Total 311 (delta 26), reused 304 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (311/311), 11.41 MiB | 19.31 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd HLCV_53/Assignment_3"
      ],
      "metadata": {
        "id": "kFREiFs17Eq9",
        "outputId": "6bd4ec4c-d10e-4aca-9a95-13164a86e781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HLCV_53/Assignment_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF4I9mNU6oxt",
        "outputId": "c40f9120-4ce3-44f7-9b5d-28a636b4ac3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (0.7.0)\n",
            "Collecting appnope==0.1.4 (from -r requirements.txt (line 2))\n",
            "  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n",
            "Collecting asttokens==3.0.0 (from -r requirements.txt (line 3))\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi==2025.4.26 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer==3.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.4.2)\n",
            "Collecting click==8.2.0 (from -r requirements.txt (line 6))\n",
            "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting comm==0.2.2 (from -r requirements.txt (line 7))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: contourpy==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.12.1)\n",
            "Collecting debugpy==1.8.14 (from -r requirements.txt (line 10))\n",
            "  Downloading debugpy-1.8.14-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting decorator==5.2.1 (from -r requirements.txt (line 11))\n",
            "  Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: docker-pycreds==0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.4.0)\n",
            "Collecting executing==2.2.0 (from -r requirements.txt (line 13))\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (3.18.0)\n",
            "Collecting fonttools==4.57.0 (from -r requirements.txt (line 15))\n",
            "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec==2025.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (2025.3.2)\n",
            "Requirement already satisfied: gitdb==4.0.12 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.0.12)\n",
            "Requirement already satisfied: GitPython==3.1.44 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (3.1.44)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.10)\n",
            "Collecting ipykernel==6.29.5 (from -r requirements.txt (line 20))\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ipython==9.2.0 (from -r requirements.txt (line 21))\n",
            "  Downloading ipython-9.2.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting ipython_pygments_lexers==1.1.1 (from -r requirements.txt (line 22))\n",
            "  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting jedi==0.19.2 (from -r requirements.txt (line 23))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (3.1.6)\n",
            "Requirement already satisfied: joblib==1.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (1.5.0)\n",
            "Collecting jupyter_client==8.6.3 (from -r requirements.txt (line 26))\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (5.7.2)\n",
            "Requirement already satisfied: kiwisolver==1.4.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (1.4.8)\n",
            "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (3.0.2)\n",
            "Collecting matplotlib==3.10.3 (from -r requirements.txt (line 30))\n",
            "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (0.1.7)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (1.3.0)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (1.6.0)\n",
            "Requirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (3.4.2)\n",
            "Collecting numpy==2.2.5 (from -r requirements.txt (line 35))\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==25.0 (from -r requirements.txt (line 36))\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (0.8.4)\n",
            "Requirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (4.9.0)\n",
            "Requirement already satisfied: pillow==11.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (11.2.1)\n",
            "Requirement already satisfied: platformdirs==4.3.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (4.3.8)\n",
            "Requirement already satisfied: prompt_toolkit==3.0.51 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (3.0.51)\n",
            "Collecting protobuf==6.31.0 (from -r requirements.txt (line 42))\n",
            "  Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting psutil==7.0.0 (from -r requirements.txt (line 43))\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (0.7.0)\n",
            "Collecting pure_eval==0.2.3 (from -r requirements.txt (line 45))\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic==2.11.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (2.11.4)\n",
            "Requirement already satisfied: pydantic_core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 47)) (2.33.2)\n",
            "Requirement already satisfied: Pygments==2.19.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 48)) (2.19.1)\n",
            "Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 49)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (2.9.0.post0)\n",
            "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 51)) (6.0.2)\n",
            "Collecting pyzmq==26.4.0 (from -r requirements.txt (line 52))\n",
            "  Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 53)) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (1.6.1)\n",
            "Requirement already satisfied: scipy==1.15.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 55)) (1.15.3)\n",
            "Collecting sentry-sdk==2.28.0 (from -r requirements.txt (line 56))\n",
            "  Downloading sentry_sdk-2.28.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: setproctitle==1.3.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 57)) (1.3.6)\n",
            "Collecting setuptools==80.7.1 (from -r requirements.txt (line 58))\n",
            "  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 59)) (1.17.0)\n",
            "Requirement already satisfied: smmap==5.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 60)) (5.0.2)\n",
            "Collecting stack-data==0.6.3 (from -r requirements.txt (line 61))\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting sympy==1.14.0 (from -r requirements.txt (line 62))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: threadpoolctl==3.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 63)) (3.6.0)\n",
            "Collecting torch==2.7.0 (from -r requirements.txt (line 64))\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision==0.22.0 (from -r requirements.txt (line 65))\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: tornado==6.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 66)) (6.4.2)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 67)) (4.67.1)\n",
            "Collecting traitlets==5.14.3 (from -r requirements.txt (line 68))\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting typing-inspection==0.4.0 (from -r requirements.txt (line 69))\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: typing_extensions==4.13.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 70)) (4.13.2)\n",
            "Requirement already satisfied: urllib3==2.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 71)) (2.4.0)\n",
            "Requirement already satisfied: wandb==0.19.11 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 72)) (0.19.11)\n",
            "Requirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 73)) (0.2.13)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0->-r requirements.txt (line 64))\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading debugpy-1.8.14-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-9.2.0-py3-none-any.whl (604 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m604.3/604.3 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.9/320.9 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.4/862.4 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.28.0-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.7/341.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/393.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:42\u001b[0m"
          ]
        }
      ],
      "source": [
        "# install requirements\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFvzwkyn6oxu"
      },
      "outputs": [],
      "source": [
        "from os.path import join as ospj\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C40bBWmD6oxu"
      },
      "outputs": [],
      "source": [
        "### If using Colab, uncomment the two following lines to mount your Google Drive.\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "### If using Colab, change the PROJECT_ROOT to where you've uploaded the project.\n",
        "### E.g. PROJECT_ROOT='/content/drive/MyDrive/TeamX/'\n",
        "### You may also need to change the `data_dir`, `save_dir`, paths in the `cfgs/exercise_3/` configs.\n",
        "\n",
        "PROJECT_ROOT='./'\n",
        "# import sys\n",
        "# sys.path.append(PROJECT_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6JfibqG6oxv"
      },
      "outputs": [],
      "source": [
        "# Just so that you don't have to restart the notebook with every change.\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO23A_d-c4t8"
      },
      "source": [
        "In Exercise 3, you will implement a convolutional neural network to perform image classification and explore methods to improve the training performance and generalization of these networks.\n",
        "We will use the CIFAR-10 dataset as a benchmark for our networks, similar to the previous exercise. This dataset consists of 50000 training images of 32x32 resolution with 10 object classes, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The task is to implement a convolutional network to classify these images using the PyTorch library. The four questions are,\n",
        "\n",
        "- Implementing a convolutional neural network, training it, and visualizing its weights (Question 1).\n",
        "- Experiment with batch normalization and early stopping (Question 2).\n",
        "- Data augmentation and dropout to improve generalization (Question 3).\n",
        "- Implement transfer learning from an ImageNet-pretrained model (Question 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5kBQ__36oxv"
      },
      "source": [
        "Before we begin, here are a few remarks regarding the codebase for this assignment.\n",
        "\n",
        "\n",
        "For every experiment, you would define a config dictionary (see the dictionary in `./cfgs/exercise_3/cnn_cifar10.py`). Every config dictionary, will have the configuration for\n",
        "- data (e.g batch size, shuffle, which DataModule to use, splitting)\n",
        "- model (e.g which class module to use and what arguments to pass to it)\n",
        "- training (e.g type of optimizer, lr_scheduler, n_epochs etc.)\n",
        "\n",
        "The DataModules are located at  `src/data_loaders/` and they inherit from a base_data_module that handles things such as splitting the data (see `src/data_loaders/base_data_modules.py`). A sample datamodule may inherit from this class (e.g `src/data_loaders/data_modules.py`). The main concern is that datamodule initialization should get everything ready, so that one can simply get the dataloaders for train/held-out sets from it (see `get_loader` and `get_heldout_loader` in BaseDataModule). The data augmentations are also done in a preset fation. One defines the preset in `utils/transform_presets.py` and simply specifies the *preset key* in the config for datamodule.\n",
        "\n",
        "The models are defined in `src/models/` (see for instance `src/models/cnn/model.py`). These are typical Pytorch nn.Modules that we had also seen in Assignment 2. They might additionally have extra methods such as `VisualizeFilter` in `model.py`.\n",
        "\n",
        "The Traier glues everything together. It creates the model, sets up optimizer, lr_schduler etc. and has the option to `train()` or `evaluate()` a model over the given dataloaders. It also logs everything in `Logs/YOUR_EXP_NAME.log` and saves the checkpoints under the `Saved/YOUR_EXP_NAME/`. Please familirize yourself with the `__init__` and methods of both `trainers/base_trainer.py` and `trainers/cnn_trainer.py` before continuing with the assignment.\n",
        "\n",
        "Lastly, for tracking different metrics (top(1/5) (train/val) accuracy or losses), we use a MetricTracker object defined in `src/utils/utils.py`. A single tracker keeps track of multiple metric keys and can `update()` their history by adding new values to a list. In the end, it can be used to return an average of a metric.\n",
        "\n",
        "\n",
        "Feel free to ask questions on the forum if part of the codebase is confusing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyHKxVkc4t-"
      },
      "source": [
        "### Question 1: Implement Convolutional Network (10 points)\n",
        "\n",
        "In this question, we will implement a five-layered convolutional neural network architecture as well as the loss function to train it. Refer to the comments in the code to the exact places where you need to fill in the code.\n",
        "\n",
        "![Failed to load the image. Please view it yourself at ./data/exercise-3/fig1_resized.png](https://github.com/MoidHuda/HLCV_53/blob/main/Assignment_3/data/exercise-3/fig1_resized.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZsjBOeJc4uA"
      },
      "source": [
        "Our architecture is shown in Fig 1. It has five convolution blocks. Each block is consist of convolution, max pooling, and ReLU operation in that order. We will use 3×3 kernels in all convolutional layers. Set the padding and stride of the convolutional layers so that they maintain the spatial dimensions. Max pooling operations are done with 2×2 kernels, with a stride of 2, thereby halving the spatial resolution each time. Finally, stacking these five blocks leads to a 512 × 1 × 1 feature map. Classification is achieved by a fully connected layer. We will train convolutional neural networks on the CIFAR-10 dataset. Implement a class ConvNet to define the model described. The ConvNet takes 32 × 32 color images as inputs and has 5 hidden layers with 128, 512, 512, 512, 512 filters, and produces a 10-class classification.\n",
        "\n",
        "a) Please implement the above network (initialization and forward pass) in class `ConvNet` in `models/cnn/model.py`. The code to train the model is already provided in the `trainers/base_trainer.py`'s train() and `trainers/cnn_trainer`'s _train_epoch(). Train the above model and report the training and validation accuracies. (5 points)\n",
        "\n",
        "b) Implement the method `__str__` in `models/base_model.py`, which should give a string representaiton of the model. The string should show the number of `trainable` parameters for each layer. This gives us a measure of model capacity. Also at the end, it should print the total number of trainable parameters for the entire model. (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyQqsQZVc4uD",
        "tags": [
          "batch norm"
        ]
      },
      "source": [
        "c) Implement a function `VisualizeFilter` in `models/cnn/model.py`, which visualizes the filters of the first convolution layer implemented in Q1.a. In other words, you need to show 128 filters with size 3x3 as color images (since each filter has three input channels). Stack these into 3x3 color images into one large image. You can use the `imshow` function from the `matplotlib` library to visualize the weights. See an example in Fig. 2\n",
        "\n",
        "![Failed to load the image. Please view it yourself at ./data/exercise-3/fig2_resized.png](https://github.com/MoidHuda/HLCV_53/blob/main/Assignment_3/data/exercise-3/fig2_resized.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUODZqzr6oxw"
      },
      "source": [
        "Compare the filters before and after training. Do you see any patterns? (3 points). Please attach your output images before and after training in a cell with your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LYwyaxA6oxw"
      },
      "outputs": [],
      "source": [
        "from cfgs.exercise_3 import cnn_cifar10\n",
        "q1_config = cnn_cifar10.q1_experiment\n",
        "\n",
        "datamodule_class = q1_config['datamodule']\n",
        "data_args = q1_config['data_args']\n",
        "\n",
        "dm = datamodule_class(**data_args)\n",
        "\n",
        "# Based on the heldout_split in the config file,\n",
        "# the datamodule will break the dataset into two splits\n",
        "train_data_loader = dm.get_loader()\n",
        "valid_data_loader = dm.get_heldout_loader()\n",
        "\n",
        "# Test loader is the same as train loader\n",
        "# except that training=False, shuffle=False, and no splitting is done\n",
        "# So we use the exact config from training and just modify these arguments\n",
        "test_data_args = deepcopy(data_args) # copy the args\n",
        "test_data_args['training'] = False\n",
        "test_data_args['shuffle'] = False\n",
        "test_data_args['heldout_split'] = 0.0\n",
        "\n",
        "# Now we initialize the test module with the modified config\n",
        "test_dm = datamodule_class(**test_data_args)\n",
        "test_loader = test_dm.get_loader() # and get the loader from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iosxXtX26oxw"
      },
      "outputs": [],
      "source": [
        "trainer_class = q1_config['trainer_module']\n",
        "trainer_cnn = trainer_class(\n",
        "    config = q1_config,\n",
        "    log_dir = ospj(PROJECT_ROOT,'Logs'),\n",
        "    train_loader=train_data_loader,\n",
        "    eval_loader=valid_data_loader,\n",
        ")\n",
        "\n",
        "trainer_cnn.model.VisualizeFilter()\n",
        "trainer_cnn.train()\n",
        "trainer_cnn.model.VisualizeFilter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXcBCIvG6oxw"
      },
      "outputs": [],
      "source": [
        "# Change this to the experiment you want to evaluate\n",
        "path = './Saved/CIFAR10_CNN/last_model.pth'\n",
        "\n",
        "trainer_cnn.load_model(path=path)\n",
        "\n",
        "result = trainer_cnn.evaluate(loader=test_loader)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKti0Y9ic4uK"
      },
      "source": [
        "#### Wirte your report for Q1 in this cell.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5_tJXTmc4uK"
      },
      "source": [
        "### Question 2: Improve training of Convolutional Networks (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FkZHgDl6oxw"
      },
      "source": [
        "a) Batch normalization is a widely used operation in neural networks, which will increase the speed of convergence and reach higher performance. You can read the paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” for more theoretical details.\n",
        "In practice, these operations are implemented in most toolboxes, such as PyTorch and TensorFlow. Add batch normalization in the model of Q1.a (You can use PyTorch's implementation). Please keep other hyperparameters the same, but only add batch normalization. The ConvNet with batch normalization still uses the same class with Q1.a but different arguments. Check the code for details. In each block, the computations should be in the order of **[convolution -> batch normalization -> pooling -> ReLU]**. Compare the loss curves and accuracy using batch normalization to its counterpart in Q1.a. (5 points)\n",
        "\n",
        "In order to run this experiment, please create a new config dictionary in `cnn_cifar10.py` under the name `q2a_normalization_experiment` (Hint: most of it should be similar to Q1's config). Don't forget to assign the config a new name, so that it doesn't overwrite previous experiments. Similar to the above cells, import the config and run the experiment.\n",
        "\n",
        "You can also add extra code to `base_trainer.py` or `cnn_trainer.py` so that they return extra information after the training is finished. For example, recall that in assignment 2's `models/twolayernet/model.py` we had a train method that would return the history of loss values, and then in the notebook the history was plotted with matplotlib. Feel free to make adjustments that let you better understand what's happening. This also applies to next questions. Right now the code only uses tensorboard and wandb for plotting (if enabled in config)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjdTyrzR6oxx"
      },
      "source": [
        "Wirte your report for Q2.a in this cell. Feel free to add extra code cells\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKsJVCJo6oxx"
      },
      "source": [
        "b) Throughout training, we optimize our parameters on the training set. This does not guarantee that with every step we also improve on validation and test set as well! Hence, there is no reason for our latest training checkpoint (the last checkpoint after the last epoch) to be the best to keep. One simple idea is to save a checkpoint of the best model for the validation set throughout the training. Meanining that as the training proceeds, we keep checking our **validation** accuracy after each epoch (or every N epochs) and save the best model. This can mitigate overfitting, as if the model overfits to training data (and accuracy on validation set drops), we would still have access to the best model checkpoint! Note that you **should not** do this on the test set, as we are not alowed to optimize **anything** (including the checkpoint selection) on the test set.\n",
        "\n",
        "For this task, you need add the logic for saving the `best model` during the training. In the `src/trainers/base_trainer`, in method `train()` we already have the call to `self.evaluate()`. All you need to add is to process the returned result (a dictionary of metric_key -> metric_value) and see if you should save a checkpoint of the model. If yes, then you can save a checkpoint at `self.checkpoint_dir` under `best_val_model.pth` or a similar name, using the `save_model()` method. Feel free to define additional class attributes or methods if needed.\n",
        "\n",
        "We also recommend adding a few prints, such as the epochs that you save the best model at. You can also use the `self.logger` object.\n",
        "\n",
        "Please also implement the `should_evaluate()` in the `trainers/base_tariner.py`, which allows for doing the cross-validation evaluation in intervals, based on the config.\n",
        "\n",
        "\n",
        "Increase the training epochs to 50 in Q1.a and Q2.a (simply edit their config dictionaries), and compare the **best model** and **latest model** on the **training set** and **validation set**. Due to the randomness, you can train multiple times to verify and observe overfitting and early stopping. (5 points)\n",
        "\n",
        "\n",
        "Feel free to add any needed train/evaluation code below for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWeRAObUc4uL"
      },
      "source": [
        "Wirte your report for Q2.b in this cell. Feel free to add extra code cells\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz8LJejg6oxx"
      },
      "source": [
        "c) While in part `b` we save the best model, we still do as many epochs as indicated in the config file. This is not convenient as the overfitting steps are wasting time and compute and also wouldn't affect the best model. Hence, Early Stopping can be helpful, where we **stop** the training after a few non-improving steps! Early stopping logic should be considered after every training epoch is finished, to see if we should do more epochs or not. Therefore, the logic should should be implemented ath the end of the loop over epochs in the `train()` method of `base_trainer.py` (which takes care of running multiple epochs).\n",
        "\n",
        "Once implemented, you need a new config dictionary to enable early stopping. Simply create a new one at the bottom of `cfgs/exercise-3/cnn_cifar10.py`. It should be mostly similar to previous config, with the following modification:\n",
        "```Python\n",
        "q2c_earlystop_experiment = dict(\n",
        "    name = 'Some New Name' # Otherwise it will overwrite previous experiment!\n",
        "    ...\n",
        "    trainer = dict(\n",
        "        ...\n",
        "        monitor = \"off\", # -> chante to \"max eval_top1\"\n",
        "        early_stop = 0, #  -> change to 4\n",
        "    ),\n",
        ")\n",
        "```\n",
        "This will enable the early stopping to be considered for `eval_top1` metric and the maximum number of non-improving steps will be set to 4.\n",
        "\n",
        "Use the cells below to re-run one of the experiments from part `b` that the best epoch was way lower than the total number of epochs, and see if early stopping can prevent unnecessary training epochs in that case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7tDsDNF6oxx"
      },
      "source": [
        "Wirte your report for Q2.c in this cell.\n",
        "\n",
        "Feel free to add extra code cells\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc1RwzMnc4uL"
      },
      "source": [
        "### Question 3: Improve generalization of Convolutional Networks (10 points)\n",
        "\n",
        "We saw in Q2 that the model can start over-fitting to the training set if we continue training for long. To prevent over-fitting, there are two main paradigms we can focus on.\n",
        "\n",
        "The first is to get more training data. This might be a difficult and expensive process. However, it is generally the most effective way to learn more general models. A cheaper alternative is to perform data augmentation. The second approach is to regularize the model to prevent overfitting.\n",
        "\n",
        "In the following sub-questions, we will experiment with each of these paradigms and measure the effect on the model generalization. We recommend disabling Early Stopping from previous question (simply removing it from config file) so that it does not interrupt your experiments with data augmentations and you maintain full control over number of epochs.\n",
        "\n",
        "\n",
        "\n",
        "a) Data augmentation is the process of creating more training data by applying certain transformations to the training set images. Usually, the underlying assumption is that the label of the image does not change under the applied transformations. This includes geometric transformations like translation, rotation, scaling, flipping, random cropping, and color transformations like greyscale, colorjitter. For every image in the training batch, a random transformation is sampled from the possible ones (e.g., a random number of pixels to translate the image by) and is applied to the image. While designing the data input pipeline, we must choose the hyper-parameters for these transformations (e.g., limits of translation or rotation) based on things we expect to see in the test-set/real world. Your task in this question is to implement the data augmentation for the CIFAR-10 classification task. Many of these transformations are implemented in the `torchvision.transforms` package. Familiarize yourself with the APIs of these transforms, and functions to compose multiple transforms or randomly sample them. Next, implement geometric and color space data augmentations for the CIFAR-10 dataset, by choosing the right functions and order of application. Tune the hyper-parameters of these data augmentations to improve the validation performance. You will need to train the model a bit longer (20-30 epochs) with data augmentation, as the training data is effectively larger now. Discuss which augmentations work well for you in the report. (6 points)\n",
        "\n",
        "Create as many config dictionaries as you need in `cnn_cifar10.py`. For every augmentation, simply create a new preset under `src/utils/transform_presets.py` and reference its name in your experiment's config dict.\n",
        "\n",
        "\n",
        "\n",
        "b) Dropout is a popular scheme to regularize the model to improve generalization. The dropout layer works by setting the input activations randomly to zero at the output. You can implement Dropout by adding the `torch.nn.Dropout` layer between the conv blocks in your model. The layer has a single hyper-parameter $p$, which is the probability of dropping the input activations. High values of $p$ regularize the model heavily and decrease model capacity, but with low values, the model might overfit. Find the right hyper-parameter for $p$ by training the model for different values of $p$ and comparing training validation and validation accuracies. You can use the same parameter $p$ for all layers. You can also disable the data augmentation from the previous step while running this experiment, to clearly see the benefit of dropout. Show the plot of training and validation accuracies for different values of dropout (0.1 - 0.9) in the report. Create as many config dictionaries as you need in `cnn_cifar10.py`. (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ-Ksf1i6oxx"
      },
      "source": [
        "Wirte your report for Q3 in this cell. Feel free to add extra code cells\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOgg_0Scc4uL"
      },
      "source": [
        "### Question 4: Use pretrained networks (10 points)\n",
        "\n",
        "It has become standard practice in computer vision tasks related to images to use a convolutional network pre-trained as the backbone feature extraction network and train new layers on top for the target task. In this question, we will implement such a model. We will use the `VGG_11_bn` network from the `torchvision.models` library as our backbone network. This model has been trained on ImageNet, achieving a top-5 error rate of 10.19%. It consists of 8 convolutional layers followed by adaptive average pooling and fully-connected layers to perform the classification. We will get rid of the average pooling and fully-connected layers from the `VGG_11_bn` model and attach our own fully connected layers to perform the CIFAR-10 classification.\n",
        "\n",
        "a) Instantiate a pretrained version of the `VGG_11_bn` model with ImageNet pre-trained weights. Add two fully connected layers on top, with Batch Norm and ReLU layers in between them, to build the CIFAR-10 10-class classifier. Note that you will need to set the correct mean and variance in the data-loader, to match the mean and variance the data was normalized with when the `VGG_11_bn` was trained. Train only the newly added layers while disabling gradients for the rest of the network. Each parameter in PyTorch has a required grad flag, which can be turned off to disable gradient computation for it. Get familiar with this gradient control mechanism in PyTorch and train the above model. As a reference point, you will see validation accuracies in the range (61-65%) if implemented correctly. (6 points)\n",
        "\n",
        "b) We can see that while the ImageNet features are useful, just learning the new layers does not yield better performance than training our own network from scratch. This is due to the domain-shift between the ImageNet dataset (224x224 resolution images) and the CIFAR-10 dataset (32x32 images). To improve the performance we can fine-tune the whole network on the CIFAR-10 dataset, starting from the ImageNet initialization (set `\"fine_tune\"` to `true` in `vgg_cifar10.py`). To do this, enable gradient computation to the rest of the network, and update all the model parameters. Additionally train a baseline model where the entire network is trained from scratch, without loading the ImageNet weights (set `\"weights\"` to `None` in `vgg_cifar10.py`). Compare the two models' training curves, validation, and testing performance in the report. (4 points)\n",
        "\n",
        "\n",
        "If you're using Pytorch 1, the `weights` argument will not work. In that case, you need to change the `weights` argument to `pretrained=True` or `False`. Feel free to post on Forum if you have any issues.\n",
        "\n",
        "For both questions, feel free to modify the data augmentation by defining a new preset and referring to it in the config file. However, make sure that in your experiments you always change only one thing at a time (i.e use the same augmentation for both method A and method B if you're comparing them with each other!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er58Xpw36oxx"
      },
      "outputs": [],
      "source": [
        "from cfgs.exercise_3 import vgg_cifar10\n",
        "q4_config = vgg_cifar10.q4_dict\n",
        "\n",
        "\n",
        "datamodule_class = q4_config['datamodule']\n",
        "data_args = q4_config['data_args']\n",
        "\n",
        "dm = datamodule_class(**data_args)\n",
        "\n",
        "# Based on the heldout_split in the config file,\n",
        "# the datamodule will break the dataset into two splits\n",
        "train_data_loader = dm.get_loader()\n",
        "valid_data_loader = dm.get_heldout_loader()\n",
        "\n",
        "# Test loader is the same as train loader\n",
        "# except that training=False, shuffle=False, and no splitting is done\n",
        "# So we use the exact config from training and just modify these arguments\n",
        "test_data_args = deepcopy(data_args) # copy the args\n",
        "test_data_args['training']=False\n",
        "test_data_args['shuffle']=False\n",
        "test_data_args['heldout_split']=0.0\n",
        "\n",
        "# Now we initialize the test module with the modified config\n",
        "test_dm = datamodule_class(**test_data_args)\n",
        "test_loader = test_dm.get_loader() # and get the loader from it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB4jMF486oxx"
      },
      "source": [
        " By default WandB is enabled in config file for `vgg_cifar10.py`. You can set it to false if you don't want to use it. It's not an essential part of the assignment anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCa6UqSZ6oxy"
      },
      "outputs": [],
      "source": [
        "wandb_enabled = q4_config['trainer_config']['wandb']\n",
        "if wandb_enabled:\n",
        "    import wandb\n",
        "\n",
        "    # change entity to your wandb username/group name. Also feel free to rename project and run names.\n",
        "    run = wandb.init(\n",
        "        project=\"HLCV-exercise-3\", # Change the project name if you wish.\n",
        "        name=q4_config['name'],\n",
        "        config=q4_config,\n",
        "        entity=\"Curious Puffin\", # Replace the curious puffin with your WandB username :)\n",
        "        dir=PROJECT_ROOT\n",
        "    )\n",
        "    run.name = run.name + f'-{run.id}'\n",
        "    assert run is wandb.run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "173pWY9E6oxy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZx8A4S66oxy"
      },
      "outputs": [],
      "source": [
        "trainer_class = q4_config['trainer_module']\n",
        "trainer_vgg = trainer_class(\n",
        "    config = q4_config,\n",
        "    log_dir = ospj(PROJECT_ROOT,'Logs'),\n",
        "    train_loader=train_data_loader,\n",
        "    eval_loader=valid_data_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iugkQrjz6oxy"
      },
      "outputs": [],
      "source": [
        "trainer_vgg.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DjaUUOu6oxy"
      },
      "outputs": [],
      "source": [
        "# Change this to the experiment you want to evaluate\n",
        "path = './Saved/CIFAR10_VGG/last_model.pth'\n",
        "\n",
        "trainer_vgg.load_model(path=path)\n",
        "\n",
        "result = trainer_vgg.evaluate(loader=test_loader)\n",
        "\n",
        "print(result)\n",
        "\n",
        "if wandb_enabled:\n",
        "    for metrics, values in result.items():\n",
        "        wandb.run.summary[f\"test_{metrics}\"] = values\n",
        "\n",
        "    # Change the title and message as you wish.\n",
        "    # Would only work if you have enabled push notifications for your email/slack in wandb account settings.\n",
        "    # Of course not an essential part of the assignment :)\n",
        "    wandb.alert(\n",
        "        title=\"Training Finished\",\n",
        "        text=f'VGG Training has finished. Test results: {result}', level=wandb.AlertLevel.INFO\n",
        "    )\n",
        "\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uvjz11dc4uN"
      },
      "source": [
        "#### Write your report for Q4 in this cell.\n",
        "Feel free to add more code cells if needed.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}